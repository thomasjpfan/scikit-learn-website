

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
   <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>2.8. Density Estimation &mdash; scikit-learn 0.22.dev0 documentation</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/density.html" />

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" /> 
</head>
<body>


<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container">
      <a class="navbar-brand" href="../index.html">
        <img
          src="../_static/scikit-learn-logo-notext.png"
          height="38"
          alt="logo"/>
      </a>
    <button
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="nav-link" href="../index.html">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../install.html">Install</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../documentation.html">Documentation</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="../auto_examples/index.html">Examples</a>
        </li>
      </ul>
      <div class="sk-search-form">
          <div class="gcse-search" id="cse" data-linktarget="_parent"></div>
      </div>
      </form>
    </div>
  </div>
</nav>
<div class="d-flex " id="sk-doc-wrapper">
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../index.html">
            <img
              src="../_static/scikit-learn-logo-small.png"
              height="38"
              alt="logo"/>
          </a>
        </div><div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
              <a href="outlier_detection.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2.7. Novelty and Outlier Detection">Prev</a><a href="../unsupervised_learning.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2. Unsupervised learning">Up</a>
              <a href="neural_networks_unsupervised.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2.9. Neural network models (unsupervised)">Next</a>
          </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.22.dev0</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Other versions</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Please <a class="font-weight-bold" href="../about.html#citing-scikit-learn"><string>cite us</string></a> if you use the software.
          </p>
        </div>
        <ul>
<li><a class="reference internal" href="#">2.8. Density Estimation</a><ul>
<li><a class="reference internal" href="#density-estimation-histograms">2.8.1. Density Estimation: Histograms</a></li>
<li><a class="reference internal" href="#kernel-density-estimation">2.8.2. Kernel Density Estimation</a></li>
</ul>
</li>
</ul>

      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body" role="main">
        
  <div class="section" id="density-estimation">
<span id="id1"></span><h1>2.8. Density Estimation<a class="headerlink" href="#density-estimation" title="Permalink to this headline">¶</a></h1>
<p>Density estimation walks the line between unsupervised learning, feature
engineering, and data modeling.  Some of the most popular and useful
density estimation techniques are mixture models such as
Gaussian Mixtures (<a class="reference internal" href="generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture" title="sklearn.mixture.GaussianMixture"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.mixture.GaussianMixture</span></code></a>), and
neighbor-based approaches such as the kernel density estimate
(<a class="reference internal" href="generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" title="sklearn.neighbors.KernelDensity"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.KernelDensity</span></code></a>).
Gaussian Mixtures are discussed more fully in the context of
<a class="reference internal" href="clustering.html#clustering"><span class="std std-ref">clustering</span></a>, because the technique is also useful as
an unsupervised clustering scheme.</p>
<p>Density estimation is a very simple concept, and most people are already
familiar with one common density estimation technique: the histogram.</p>
<div class="section" id="density-estimation-histograms">
<h2>2.8.1. Density Estimation: Histograms<a class="headerlink" href="#density-estimation-histograms" title="Permalink to this headline">¶</a></h2>
<p>A histogram is a simple visualization of data where bins are defined, and the
number of data points within each bin is tallied.  An example of a histogram
can be seen in the upper-left panel of the following figure:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_kde_1d.html"><img alt="hist_to_kde" src="../_images/sphx_glr_plot_kde_1d_0011.png" style="width: 512.0px; height: 384.0px;" /></a></strong></p><p>A major problem with histograms, however, is that the choice of binning can
have a disproportionate effect on the resulting visualization.  Consider the
upper-right panel of the above figure.  It shows a histogram over the same
data, with the bins shifted right.  The results of the two visualizations look
entirely different, and might lead to different interpretations of the data.</p>
<p>Intuitively, one can also think of a histogram as a stack of blocks, one block
per point.  By stacking the blocks in the appropriate grid space, we recover
the histogram.  But what if, instead of stacking the blocks on a regular grid,
we center each block on the point it represents, and sum the total height at
each location?  This idea leads to the lower-left visualization.  It is perhaps
not as clean as a histogram, but the fact that the data drive the block
locations mean that it is a much better representation of the underlying
data.</p>
<p>This visualization is an example of a <em>kernel density estimation</em>, in this case
with a top-hat kernel (i.e. a square block at each point).  We can recover a
smoother distribution by using a smoother kernel.  The bottom-right plot shows
a Gaussian kernel density estimate, in which each point contributes a Gaussian
curve to the total.  The result is a smooth density estimate which is derived
from the data, and functions as a powerful non-parametric model of the
distribution of points.</p>
</div>
<div class="section" id="kernel-density-estimation">
<span id="kernel-density"></span><h2>2.8.2. Kernel Density Estimation<a class="headerlink" href="#kernel-density-estimation" title="Permalink to this headline">¶</a></h2>
<p>Kernel density estimation in scikit-learn is implemented in the
<a class="reference internal" href="generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" title="sklearn.neighbors.KernelDensity"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.KernelDensity</span></code></a> estimator, which uses the
Ball Tree or KD Tree for efficient queries (see <a class="reference internal" href="neighbors.html#neighbors"><span class="std std-ref">Nearest Neighbors</span></a> for
a discussion of these).  Though the above example
uses a 1D data set for simplicity, kernel density estimation can be
performed in any number of dimensions, though in practice the curse of
dimensionality causes its performance to degrade in high dimensions.</p>
<p>In the following figure, 100 points are drawn from a bimodal distribution,
and the kernel density estimates are shown for three choices of kernels:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_kde_1d.html"><img alt="kde_1d_distribution" src="../_images/sphx_glr_plot_kde_1d_0031.png" style="width: 512.0px; height: 384.0px;" /></a></strong></p><p>It’s clear how the kernel shape affects the smoothness of the resulting
distribution.  The scikit-learn kernel density estimator can be used as
follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors.kde</span> <span class="k">import</span> <span class="n">KernelDensity</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kde</span> <span class="o">=</span> <span class="n">KernelDensity</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;gaussian&#39;</span><span class="p">,</span> <span class="n">bandwidth</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kde</span><span class="o">.</span><span class="n">score_samples</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,</span>
<span class="go">       -0.41076071])</span>
</pre></div>
</div>
<p>Here we have used <code class="docutils literal notranslate"><span class="pre">kernel='gaussian'</span></code>, as seen above.
Mathematically, a kernel is a positive function <span class="math notranslate nohighlight">\(K(x;h)\)</span>
which is controlled by the bandwidth parameter <span class="math notranslate nohighlight">\(h\)</span>.
Given this kernel form, the density estimate at a point <span class="math notranslate nohighlight">\(y\)</span> within
a group of points <span class="math notranslate nohighlight">\(x_i; i=1\cdots N\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\rho_K(y) = \sum_{i=1}^{N} K((y - x_i) / h)\]</div>
<p>The bandwidth here acts as a smoothing parameter, controlling the tradeoff
between bias and variance in the result.  A large bandwidth leads to a very
smooth (i.e. high-bias) density distribution.  A small bandwidth leads
to an unsmooth (i.e. high-variance) density distribution.</p>
<p><a class="reference internal" href="generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity" title="sklearn.neighbors.KernelDensity"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.KernelDensity</span></code></a> implements several common kernel
forms, which are shown in the following figure:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_kde_1d.html"><img alt="kde_kernels" src="../_images/sphx_glr_plot_kde_1d_0021.png" style="width: 512.0px; height: 384.0px;" /></a></strong></p><p>The form of these kernels is as follows:</p>
<ul>
<li><p>Gaussian kernel (<code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">'gaussian'</span></code>)</p>
<p><span class="math notranslate nohighlight">\(K(x; h) \propto \exp(- \frac{x^2}{2h^2} )\)</span></p>
</li>
<li><p>Tophat kernel (<code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">'tophat'</span></code>)</p>
<p><span class="math notranslate nohighlight">\(K(x; h) \propto 1\)</span> if <span class="math notranslate nohighlight">\(x &lt; h\)</span></p>
</li>
<li><p>Epanechnikov kernel (<code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">'epanechnikov'</span></code>)</p>
<p><span class="math notranslate nohighlight">\(K(x; h) \propto 1 - \frac{x^2}{h^2}\)</span></p>
</li>
<li><p>Exponential kernel (<code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">'exponential'</span></code>)</p>
<p><span class="math notranslate nohighlight">\(K(x; h) \propto \exp(-x/h)\)</span></p>
</li>
<li><p>Linear kernel (<code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">'linear'</span></code>)</p>
<p><span class="math notranslate nohighlight">\(K(x; h) \propto 1 - x/h\)</span> if <span class="math notranslate nohighlight">\(x &lt; h\)</span></p>
</li>
<li><p>Cosine kernel (<code class="docutils literal notranslate"><span class="pre">kernel</span> <span class="pre">=</span> <span class="pre">'cosine'</span></code>)</p>
<p><span class="math notranslate nohighlight">\(K(x; h) \propto \cos(\frac{\pi x}{2h})\)</span> if <span class="math notranslate nohighlight">\(x &lt; h\)</span></p>
</li>
</ul>
<p>The kernel density estimator can be used with any of the valid distance
metrics (see <a class="reference internal" href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.neighbors.DistanceMetric</span></code></a> for a list of available metrics), though
the results are properly normalized only for the Euclidean metric.  One
particularly useful metric is the
<a class="reference external" href="https://en.wikipedia.org/wiki/Haversine_formula">Haversine distance</a>
which measures the angular distance between points on a sphere.  Here
is an example of using a kernel density estimate for a visualization
of geospatial data, in this case the distribution of observations of two
different species on the South American continent:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_species_kde.html"><img alt="species_kde" src="../_images/sphx_glr_plot_species_kde_0011.png" style="width: 512.0px; height: 384.0px;" /></a></strong></p><p>One other useful application of kernel density estimation is to learn a
non-parametric generative model of a dataset in order to efficiently
draw new samples from this generative model.
Here is an example of using this process to
create a new set of hand-written digits, using a Gaussian kernel learned
on a PCA projection of the data:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_digits_kde_sampling.html"><img alt="digits_kde" src="../_images/sphx_glr_plot_digits_kde_sampling_0011.png" style="width: 512.0px; height: 384.0px;" /></a></strong></p><p>The “new” data consists of linear combinations of the input data, with weights
probabilistically drawn given the KDE model.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_kde_1d.html#sphx-glr-auto-examples-neighbors-plot-kde-1d-py"><span class="std std-ref">Simple 1D Kernel Density Estimation</span></a>: computation of simple kernel
density estimates in one dimension.</p></li>
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_digits_kde_sampling.html#sphx-glr-auto-examples-neighbors-plot-digits-kde-sampling-py"><span class="std std-ref">Kernel Density Estimation</span></a>: an example of using
Kernel Density estimation to learn a generative model of the hand-written
digits data, and drawing new samples from this model.</p></li>
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_species_kde.html#sphx-glr-auto-examples-neighbors-plot-species-kde-py"><span class="std std-ref">Kernel Density Estimate of Species Distributions</span></a>: an example of Kernel Density
estimation using the Haversine distance metric to visualize geospatial data</p></li>
</ul>
</div>
</div>
</div>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2019, scikit-learn developers (BSD License).
          <a href="../_sources/modules/density.rst.txt" rel="nofollow">Show this page source</a>
      </footer>
    </div>
  </div>
  <button class="sk-btn-toggle-toc btn sk-btn-primary" id="menu-toggle">Toggle Menu</button>
</div>
<script src="../_static/js/vendor/jquery.min.js"></script>
<script src="../_static/js/vendor/bootstrap.min.js"></script>


<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    $("#menu-toggle").click(function(e) {
        e.preventDefault();
        $("#sk-doc-wrapper").toggleClass("toggled");
    });

    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Had navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  hashTargetOnTop = function() {
    var hash = window.location.hash;
    if ( hash.length < 2 ) { return false; }

    var target = document.getElementById( hash.slice(1) );
    if ( target === null ) { return false; }

    return target.getBoundingClientRect().top < 2;
  };

  // Hide navbar on load if hash target is on top
  var navBar = document.getElementById("navbar")
  if (hashTargetOnTop()) {
      navBar.style.top = "-48px";
  }

  var prevScrollpos = window.pageYOffset;
  hideOnScroll = function(lastScrollTop) {
    var currentScrollPos = window.pageYOffset;
    if (lastScrollTop > 2 && (prevScrollpos <= currentScrollPos) || hashTargetOnTop()){
      navBar.style.top = "-48px";
    } else {
      navBar.style.top = "0";
    }
    prevScrollpos = currentScrollPos;
  };

  /*** high preformance scroll event listener***/
  var raf = window.requestAnimationFrame ||
      window.webkitRequestAnimationFrame ||
      window.mozRequestAnimationFrame ||
      window.msRequestAnimationFrame ||
      window.oRequestAnimationFrame;
  var $window = $(window);
  var lastScrollTop = $window.scrollTop();

  if (raf) {
      loop();
  }

function loop() {
    var scrollTop = $window.scrollTop();
    if (lastScrollTop === scrollTop) {
        raf(loop);
        return;
    } else {
        lastScrollTop = scrollTop;
        hideOnScroll(lastScrollTop);
        raf(loop);
    }
  }
});

</script>
<script>
    (function() {
        var cx = '016639176250731907682:tjtqbvtvij0';
        var gcse = document.createElement('script'); gcse.type = 'text/javascript'; gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(gcse, s);
    })();
</script>
    
<script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_SVG"></script>
    
</body>
</html>